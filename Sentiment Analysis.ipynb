{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "304e37a2",
      "metadata": {
        "id": "304e37a2"
      },
      "source": [
        "#  Word vectors\n",
        "\n",
        "In this two part assignment you will first examine and interact with word vectors. (This part of the assignment is adapted from a recent CS224N assignment at Stanford.) You will then implement a new approach to sentiment analysis.\n",
        "\n",
        "In this assignment we will use [gensim](https://radimrehurek.com/gensim/) to access and interact with word embeddings. In gensim we’ll be working with a KeyedVectors object which represents word embeddings. [Documentation for KeyedVectors is available.](https://radimrehurek.com/gensim/models/keyedvectors.html) However, this assignment description and the sample code in it might be sufficient to show you how to use a KeyedVectors object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0ffb28a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ffb28a8",
        "outputId": "d63bdebc-e94f-4f3f-b3fc-a8195c8ccaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "#hanieh ghabelialla  \n",
        "import gensim.downloader\n",
        "model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192b65d9",
      "metadata": {
        "id": "192b65d9"
      },
      "source": [
        "# Part 1: Examining word vectors\n",
        "\n",
        "## Polysemy and homonymy\n",
        "\n",
        "Polysemy and homonymy are the phenomena of words having multiple meanings/senses. The nearest neighbours (under cosine similarity) for a given word can indicate whether it has multiple senses.\n",
        "\n",
        "Consider the following example which shows the top-10 most similar words for *mouse*. The \"input device\" and \"animal\" senses of *mouse* are clearly visible from the top-10 most similar words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a32120e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a32120e3",
        "outputId": "72f326ad-529d-4f97-a4d6-042d70a99d2e",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('mice', 0.7038448452949524),\n",
              " ('rat', 0.6446240544319153),\n",
              " ('rodent', 0.6280483603477478),\n",
              " ('Mouse', 0.6180493831634521),\n",
              " ('cursor', 0.6154769062995911),\n",
              " ('keyboard', 0.6149151921272278),\n",
              " ('rabbit', 0.607288658618927),\n",
              " ('cat', 0.6070616245269775),\n",
              " ('joystick', 0.5888146162033081),\n",
              " ('touchpad', 0.5878496766090393)]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find words most similar using cosine similarity to \"mouse\".\n",
        "# restrict_vocab=100000 limits the results to most frequent\n",
        "# 100000 words. This avoids rare words in the output. For this\n",
        "# assignment, whenever you call most_simlilar, also pass\n",
        "# restrict_vocab=100000.\n",
        "model.most_similar('mouse', restrict_vocab=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f34e2ac",
      "metadata": {
        "id": "2f34e2ac"
      },
      "source": [
        "*Cursor*, *keyboard*, *joystick*, *touchpad* correspond to the input device sense. *Rat*, *rodent*, *rabbit*, *cat* correspond to the animal sense.\n",
        "\n",
        "\n",
        "You can observe something similar for the different senses of the word *leaves*. Find a new example that exhibits polysemy/homonymy, show its top-10 most similar words, and explain why they show that this word has multiple senses. Write your answer in the code and text boxes below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f0194298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0194298",
        "outputId": "e8605373-4c7b-4e49-c527-6ec39b002e4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Matches', 0.7960425019264221),\n",
              " ('Matching', 0.7419515252113342),\n",
              " ('Score', 0.6290581226348877),\n",
              " ('Game', 0.6225259900093079),\n",
              " ('Test', 0.6177693605422974),\n",
              " ('Play', 0.6137019991874695),\n",
              " ('Man', 0.6104843616485596),\n",
              " ('Knockout', 0.6100562810897827),\n",
              " ('Round', 0.6062699556350708),\n",
              " ('Fight', 0.6050064563751221)]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('Match', restrict_vocab=100000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86905a9c",
      "metadata": {
        "id": "86905a9c"
      },
      "source": [
        "\n",
        "answer : The word \"match\" has different meanings, such as a sports game, a tool for starting a fire, or a pairing that fits well. When we use a model to find similar words, it shows a mix of these meanings because it learned the word's usage from many different contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e90ead9",
      "metadata": {
        "id": "6e90ead9"
      },
      "source": [
        "## Synonyms and antonyms\n",
        "\n",
        "Find three words (w1 , w2 , w3) such that w1 and w2 are synonyms (i.e., have roughly the same meaning), and w1 and w3 are antonyms (i.e., have opposite meanings), but the similarity between w1 and w3 > the similarity between w1 and w2. Note that this should be counter to your expectations, because synonyms (which mean roughly the same thing) would be expected to be more similar than antonyms (which have opposite meanings). Explain why you think this unexpected situation might have occurred.\n",
        "\n",
        "Here is an example. w1 = *happy*, w2 = *cheerful*, and w3 = *sad*. (You will need to find a different example for your report.) Notice that the antonyms *happy* and *sad* are (slightly) more similar than the (near) synonyms *happy* and *cheerful*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "45f158c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45f158c0",
        "outputId": "970306ca-6ac3-4965-bfb2-823c77d848fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.68476284"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the cosine similarity between \"happy\" and \"cheerful\"\n",
        "model.similarity('happy', 'cheerful')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b5fd9873",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5fd9873",
        "outputId": "c3711bc2-f5e3-4d67-ad8f-5d7537eec25b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.69010293"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.similarity('happy', 'sad')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a061ca65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a061ca65",
        "outputId": "8da0db26-a326-4f8e-93b9-cb1ece7a0707"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4755332"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"complex\", \"Complicated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b9dd9669",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9dd9669",
        "outputId": "366dc8ef-e513-44b6-8057-24fde53b21dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.64020914"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"complex\",\"simple\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c15629",
      "metadata": {
        "id": "05c15629"
      },
      "source": [
        " answer : this situation could occur because words are often defined by their opposites in various texts, leading to their frequent co-occurrence within similar contexts, which in turn causes the embedding algorithm to place them closer together in the vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb72540c",
      "metadata": {
        "id": "eb72540c"
      },
      "source": [
        "## Analogies\n",
        "\n",
        "Analogies such as man is to king as woman is to X can be solved using word embeddings. This analogy can be expressed as X = woman + king − man. The following code snippet shows how to solve this analogy with gensim. Notice that the model gets it correct! I.e., *queen* is the most similar word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "24c757c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c757c6",
        "outputId": "4f50ee50-4ce1-410b-91e6-c3c7f5b41181"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.7786749005317688),\n",
              " ('monarch', 0.6666999459266663),\n",
              " ('princess', 0.653827428817749),\n",
              " ('kings', 0.6497675180435181),\n",
              " ('queens', 0.6284460425376892),\n",
              " ('prince', 0.6235989928245544),\n",
              " ('ruler', 0.5971586108207703),\n",
              " ('kingship', 0.5883600115776062),\n",
              " ('lady', 0.5851913094520569),\n",
              " ('royal', 0.5821066498756409)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the model's predictions for the solution to the analogy\n",
        "# \"man\" is to \"king\" as \"woman\" is to X\n",
        "model.most_similar(positive=['woman', 'king'],\n",
        "                   negative=['man'],\n",
        "                   restrict_vocab=100000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0416e879",
      "metadata": {
        "id": "0416e879"
      },
      "source": [
        "### Correct analogy\n",
        "\n",
        "Find a new analogy that the model is able to answer correctly (i.e., the most-similar word is the solution to the analogy). Explain briefly why the analogy holds. For the above example, this explanation would be something along the lines of a king is a ruler who is a man and a queen is a ruler who is a woman.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f75da5f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f75da5f5",
        "outputId": "96244d89-43a6-44ff-8544-1c9f0d90363e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('businesswoman', 0.8535027503967285),\n",
              " ('businessperson', 0.7498276233673096),\n",
              " ('entrepreneur', 0.7154306769371033),\n",
              " ('housewife', 0.6567320227622986),\n",
              " ('philanthropist', 0.6523854732513428),\n",
              " ('industrialist', 0.6522493958473206),\n",
              " ('banker', 0.6502622365951538),\n",
              " ('politician', 0.6422467231750488),\n",
              " ('lawyer', 0.6274409890174866),\n",
              " ('congresswoman', 0.6196478605270386)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"man\" is to \"businessman\" as \"woman\" is to X\n",
        "model.most_similar(positive=['woman', 'businessman'],\n",
        "                   negative=['man'],\n",
        "                   restrict_vocab=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdcdf5fa",
      "metadata": {
        "id": "cdcdf5fa"
      },
      "source": [
        "answer : The analogy holds because businessman refers to a man engaged in business activities, and by changing the gender to woman while keeping the occupation aspect constant, we look for the female equivalent, which is businesswoman."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7137368b",
      "metadata": {
        "id": "7137368b"
      },
      "source": [
        "### Incorrect analogy\n",
        "\n",
        "Find a new analogy that the model is not able to answer correctly. Again explain briefly why the analogy holds. For example, here is an analogy that the model does not answer correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "_N0K7WejrxZ8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N0K7WejrxZ8",
        "outputId": "1026da4b-f88c-490d-bdf2-e6695be80bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cups', 0.5481787919998169),\n",
              " ('coffee', 0.5461026430130005),\n",
              " ('beverage', 0.5460603833198547),\n",
              " ('drink', 0.5451807975769043),\n",
              " ('tea', 0.53434818983078),\n",
              " ('foods', 0.5310320854187012),\n",
              " ('drinks', 0.516447901725769),\n",
              " ('beverages', 0.5022991299629211),\n",
              " ('milk', 0.4976045787334442),\n",
              " ('non-food', 0.4929129481315613)]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the model's predictions for the solution to the analogy\n",
        "# \"plate\" is to \"food\" as \"cup\" is to X\n",
        "model.most_similar(positive=['cup', 'food'],\n",
        "                   negative=['plate'],\n",
        "                   restrict_vocab=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BWFSM1IUs-d_",
      "metadata": {
        "id": "BWFSM1IUs-d_"
      },
      "source": [
        "A plate is used to serve food as a cup is used to serve a drink, but the model does not predict drink, or a similar term, as the most similar word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "M-52qerwrQja",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-52qerwrQja",
        "outputId": "1cfa389e-4611-4ff6-9961-31fb36b6d001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('inches', 0.5423212647438049),\n",
              " ('hands', 0.5274760723114014),\n",
              " ('foot', 0.5268399715423584),\n",
              " ('glove', 0.5170854926109314),\n",
              " ('meters', 0.4980737268924713),\n",
              " ('fingertips', 0.4865354895591736),\n",
              " ('elbows', 0.4857611656188965),\n",
              " ('centimeters', 0.47440657019615173),\n",
              " ('metres', 0.47025373578071594),\n",
              " ('millimeters', 0.4700991213321686)]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"sock\" is to \"feet\" as \"glove\" is to X\n",
        "model.most_similar(positive=['gloves', 'feet'],\n",
        "                   negative=['socks'],\n",
        "                   restrict_vocab=100000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c70dad5",
      "metadata": {
        "id": "7c70dad5"
      },
      "source": [
        "socks is used for feet as gloves  used for hands but the model does not predict *hands*, or a similar term, as the most similar word.The model might not answer correctly due to varying associations with these words in its training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5dae136",
      "metadata": {
        "id": "e5dae136"
      },
      "source": [
        "## Bias\n",
        "\n",
        "Consider the examples below. The first shows the words that are most similar to *man* and *worker* and least similar to *woman*. The second shows the words that are most similar to *woman* and *worker* and least similar to *man*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "79b1ccfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79b1ccfd",
        "outputId": "c6785ef6-b73c-409c-d4e9-e6c2243e2dcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('workman', 0.7217649817466736),\n",
              " ('laborer', 0.6744564175605774),\n",
              " ('labourer', 0.6498093605041504),\n",
              " ('workers', 0.6487939357757568),\n",
              " ('foreman', 0.6226886510848999),\n",
              " ('machinist', 0.6098095178604126),\n",
              " ('employee', 0.6091086864471436),\n",
              " ('technician', 0.6029269099235535),\n",
              " ('helper', 0.5994961261749268),\n",
              " ('manager', 0.5832769274711609)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the words that are most similar to \"man\" and \"worker\" and\n",
        "# least similar to \"woman\".\n",
        "model.most_similar(positive=['man', 'worker'],\n",
        "                   negative=['woman'],\n",
        "                   restrict_vocab=100000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e8OM-rkHxutX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8OM-rkHxutX",
        "outputId": "6ceba839-bab9-4492-9a52-60373588b72c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('workers', 0.6522067189216614),\n",
              " ('employee', 0.6391042470932007),\n",
              " ('housewife', 0.608704686164856),\n",
              " ('nurse', 0.5983445644378662),\n",
              " ('employer', 0.5973758101463318),\n",
              " ('caseworker', 0.5940523147583008),\n",
              " ('seamstress', 0.581404447555542),\n",
              " ('laborer', 0.5809912085533142),\n",
              " ('policewoman', 0.5767977237701416),\n",
              " ('Worker', 0.5750083327293396)]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the words that are most similar to \"man\" and \"worker\" and\n",
        "# least similar to \"woman\".\n",
        "model.most_similar(positive=['woman', 'worker'],\n",
        "                   negative=['man'],\n",
        "                   restrict_vocab=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ib2xqf6y3a7z",
      "metadata": {
        "id": "Ib2xqf6y3a7z"
      },
      "source": [
        "The output shows that *man* is associated with some stereotypically male jobs (e.g., foreman, machinist) while *woman* is associated with some stereotypically female jobs (e.g., housewife, nurse, seamstress). This indicates that there is gender bias in the word embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "GiQiN5yNwuhO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiQiN5yNwuhO",
        "outputId": "02f24d52-6816-4888-c4d8-0c70440ef63f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('sports', 0.7389599084854126),\n",
              " ('netball', 0.6590258479118347),\n",
              " ('athletics', 0.6529361009597778),\n",
              " ('sporting', 0.6372365355491638),\n",
              " ('athlete', 0.5908907651901245),\n",
              " ('sportsperson', 0.5895546078681946),\n",
              " ('gymnastics', 0.5854197144508362),\n",
              " ('volleyball', 0.5823972821235657),\n",
              " ('soccer', 0.5809586644172668),\n",
              " ('sportive', 0.5755864977836609)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the words that are most similar to \"woman\" and \"sport\" and\n",
        "# least similar to \"man\".\n",
        "model.most_similar(positive=['woman', 'sport'],\n",
        "                   negative=['man'],\n",
        "                   restrict_vocab=100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "cd781f83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd781f83",
        "outputId": "8885a0d4-0da7-48d0-ad51-75d11c66c6cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('sportsman', 0.677655041217804),\n",
              " ('sporting', 0.6111401915550232),\n",
              " ('sportive', 0.5944186449050903),\n",
              " ('sports', 0.5667303204536438),\n",
              " ('football', 0.5594378709793091),\n",
              " ('showman', 0.552361011505127),\n",
              " ('athlete', 0.5520541667938232),\n",
              " ('sportsperson', 0.5415318608283997),\n",
              " ('racing', 0.5353881120681763),\n",
              " ('beast', 0.5351638197898865)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar(positive=[ 'man', 'sport'],\n",
        "                   negative=['women'],\n",
        "                   restrict_vocab=100000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ec7446",
      "metadata": {
        "id": "e0ec7446"
      },
      "source": [
        "The output shows that some sports are associated more with men, like football, while other sports are associated with women, like volleyball, indicating a potential bias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96583e72",
      "metadata": {
        "id": "96583e72"
      },
      "source": [
        "# Part 2: Sentiment Analysis\n",
        "\n",
        "## Background and data\n",
        "\n",
        "In this part you will consider sentiment analysis of tweets. You will need the data for this assignmnet from D2L: train.docs.txt. train.classes.txt, test.docs.txt, test.classes.txt. Put those files in the same directory that you run this notebook from.\n",
        "\n",
        "train.docs.txt and test.docs.txt are training and testing tweets, respectively, in one-tweet-per-line format. These are tweets related to health care reform in the United States from early 2010. All tweets contain the hashtag #hcr. These tweets have been manually labeled as “positive”, “negative”, or “neutral”.\n",
        "\n",
        "These are real tweets. Some of the tweets contain content that you might find offensive (e.g., expletives, racist and homophobic remarks). Despite this offensive content, these tweets are still very valuable data, and building NLP systems that can operate over them is important. That is why we are working with this potentially-offensive data in this assignment.\n",
        "\n",
        "This dataset is further described in the following paper: Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. [Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph](https://aclanthology.org/W11-2207/). In Proceedings of the First Workshop on Unsupervised Methods in NLP. Edinburgh, Scotland.\n",
        "\n",
        "train.classes.txt and test.classes.txt contain class labels for the training and test data, 1 label per line. The labels are “positive”, “neutral”, and “negative”.\n",
        "\n",
        "## Approach\n",
        "\n",
        "We will consider sentiment analysis using an average of word embeddings document representation and a multinomial logistic regression classifier. We will compare this approach to a most-frequent class baseline.\n",
        "\n",
        "Complete the function `vec_for_doc` below. (You should not modify other parts of the\n",
        "code.) This function takes a list consisting of the tokens in a document $d$. It then returns a vector $\\vec{v}$ representing the document as the average of the embeddings for the words in the document as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "d = w_1, w_2, ... w_n\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\vec{v} = \\dfrac{\\vec{w_1} + \\vec{w_2} + ... + \\vec{w_n}}{n}\\\\\n",
        "\\end{equation}\n",
        "\n",
        "You can then run the code to compare logistic regression using an average of word embeddings to a most-frequent class baseline. (If your implementation of `vec_for_doc` is correct, logistic regression should be the baseline in terms of accuracy (by a little bit) and in terms of F1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "XRfh_r0yB06H",
      "metadata": {
        "id": "XRfh_r0yB06H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement this function. tokenized_doc is a list of tokens in\n",
        "# a document. Return a vector representation of the document as\n",
        "# described above.\n",
        "# Hints:\n",
        "# -You can get the vector for a word w using model[w] or\n",
        "#  model.get_vector(w)\n",
        "# -You can add vectors using + and sum, e.g.,\n",
        "#  model['cat'] + model['dog']\n",
        "#  sum([model['cat'], model['dog']])\n",
        "# -You can see the shape of a vector using model['cat'].shape\n",
        "# -The vector you return should have the same shape as a word vector\n",
        "def vec_for_doc(tokenized_doc):\n",
        "\n",
        "    doc_vector = np.zeros(300)\n",
        "\n",
        "    token_count = 0\n",
        "\n",
        "    for token in tokenized_doc:\n",
        "\n",
        "        try:\n",
        "            doc_vector += model.get_vector(token)\n",
        "            token_count += 1\n",
        "        except KeyError:\n",
        "\n",
        "            continue\n",
        "    if token_count > 0:\n",
        "        return doc_vector / token_count\n",
        "    else:\n",
        "\n",
        "        return np.zeros(model.vector_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3b69d0a7",
      "metadata": {
        "id": "3b69d0a7"
      },
      "outputs": [],
      "source": [
        "import math, re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Get the train and test documents and classes. File formats\n",
        "# are similar to assignment 2.\n",
        "train_texts_fname = 'train.docs.txt'\n",
        "train_klasses_fname = 'train.classes.txt'\n",
        "test_texts_fname = 'test.docs.txt'\n",
        "test_klasses_fname = 'test.classes.txt'\n",
        "\n",
        "train_texts = [x.strip() for x in open(train_texts_fname,\n",
        "                                       encoding='utf8')]\n",
        "train_klasses = [x.strip() for x in open(train_klasses_fname,\n",
        "                                         encoding='utf8')]\n",
        "test_texts = [x.strip() for x in open(test_texts_fname,\n",
        "                                      encoding='utf8')]\n",
        "test_klasses = [x.strip() for x in open(test_klasses_fname,\n",
        "                                        encoding='utf8')]\n",
        "\n",
        "# A simple tokenizer. Applies case folding\n",
        "def tokenize(s):\n",
        "    tokens = s.lower().split()\n",
        "    trimmed_tokens = []\n",
        "    for t in tokens:\n",
        "        if re.search('\\w', t):\n",
        "            # t contains at least 1 alphanumeric character\n",
        "            t = re.sub('^\\W*', '', t) # trim leading non-alphanumeric chars\n",
        "            t = re.sub('\\W*$', '', t) # trim trailing non-alphanumeric chars\n",
        "        trimmed_tokens.append(t)\n",
        "    return trimmed_tokens\n",
        "\n",
        "# train_vecs and test_vecs are lists; each element is a vector\n",
        "# representing a (train or test) document\n",
        "train_vecs = [vec_for_doc(tokenize(x)) for x in train_texts]\n",
        "test_vecs = [vec_for_doc(tokenize(x)) for x in test_texts]\n",
        "\n",
        "# Train logistic regression, similarly to assignment 2\n",
        "lr = LogisticRegression(multi_class='multinomial',\n",
        "                        solver='sag',\n",
        "                        penalty='l2',\n",
        "                        max_iter=1000000,\n",
        "                        random_state=0)\n",
        "lr = LogisticRegression()\n",
        "clf = lr.fit(train_vecs, train_klasses)\n",
        "results = clf.predict(test_vecs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "cb5da740",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb5da740",
        "outputId": "cdcb7a52-b193-4abb-c48d-cf4cdd21a7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6975\n",
            "Macro F1:  0.3709784548964257\n"
          ]
        }
      ],
      "source": [
        "# Determine accuracy and macro F1 using sklearn evaluation metrics\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "acc = sklearn.metrics.accuracy_score(test_klasses, results)\n",
        "f1 = sklearn.metrics.f1_score(test_klasses, results, average='macro')\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Macro F1: \", f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "076ffb64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "076ffb64",
        "outputId": "f95a6ec3-ec5b-4f52-e10f-b2c589429639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline accuracy:  0.67\n",
            "Baseline macro F1:  0.26746506986027946\n"
          ]
        }
      ],
      "source": [
        "# Also determine accuracy and macro F1 for a most-frequent class baseline\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "baseline_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "baseline_clf.fit(train_vecs, train_klasses)\n",
        "baseline_results = baseline_clf.predict(test_vecs)\n",
        "\n",
        "acc = sklearn.metrics.accuracy_score(test_klasses, baseline_results)\n",
        "f1 = sklearn.metrics.f1_score(test_klasses, baseline_results, average='macro')\n",
        "\n",
        "print(\"Baseline accuracy: \", acc)\n",
        "print(\"Baseline macro F1: \", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2f4984",
      "metadata": {
        "id": "fa2f4984"
      },
      "source": [
        "# Submitting your work\n",
        "\n",
        "When you're done, submit a3.ipynb to the assignment 3 folder on D2L."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
